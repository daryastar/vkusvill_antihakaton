{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Analysis of trades.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Маркировка "
      ],
      "metadata": {
        "id": "UH-A7kPLIIAw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y51xBhlgz4J1",
        "outputId": "5c5e7136-bdd5-45f9-c156-e1949e2d73e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#@markdown <br><center><img src='https://upload.wikimedia.org/wikipedia/commons/thumb/d/da/Google_Drive_logo.png/600px-Google_Drive_logo.png' height=\"50\" alt=\"Gdrive-logo\"/></center>\n",
        "#@markdown <center><h3>Mount GDrive to /content/drive</h3></center><br>\n",
        "MODE = \"MOUNT\" #@param [\"MOUNT\", \"UNMOUNT\"]\n",
        "#Mount your Gdrive! \n",
        "from google.colab import drive\n",
        "drive.mount._DEBUG = False\n",
        "if MODE == \"MOUNT\":\n",
        "  drive.mount('/content/drive', force_remount=True)\n",
        "elif MODE == \"UNMOUNT\":\n",
        "  try:\n",
        "    drive.flush_and_unmount()\n",
        "  except ValueError:\n",
        "    pass\n",
        "  get_ipython().system_raw(\"rm -rf /root/.config/Google/DriveFS\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn. linear_model import LinearRegression\n",
        "import matplotlib.pyplot as plt\n",
        "from pandas.tseries.offsets import DateOffset"
      ],
      "metadata": {
        "id": "zhsT1Tcw0Ojo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_febr = pd.read_csv('/content/drive/MyDrive/Vkusvill/1_febr.csv', sep = ';')\n",
        "data_march = pd.read_csv('/content/drive/MyDrive/Vkusvill/1_march.csv', sep = ';')\n",
        "data_april = pd.read_csv('/content/drive/MyDrive/Vkusvill/1_april.csv', sep = ';')\n",
        "data_may = pd.read_csv('/content/drive/MyDrive/Vkusvill/1_may.csv', sep = ';')"
      ],
      "metadata": {
        "id": "2ZsAtFNt0Vn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#delete some points for equal length in all files\n",
        "febr = data_febr.loc[(data_febr['id_tt_cl'] != 15656) & (data_febr['id_tt_cl'] != 16250) & (data_febr['id_tt_cl'] != 11217)]\n",
        "march = data_march.loc[(data_march['id_tt_cl'] != 11217) & (data_march['id_tt_cl'] != 16250)]\n",
        "#make sure it works right \n",
        "mask = np.isin(febr['id_tt_cl'].unique(), march['id_tt_cl'].unique())\n",
        "print(mask)"
      ],
      "metadata": {
        "id": "Lnd5jLiq06TF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#febr.groupby('id_tt_cl').date_ch.count()"
      ],
      "metadata": {
        "id": "s08Keq7w1qzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#select dates in certain range and then group by id_tt\n",
        "#but firstly convert dates in the right format\n",
        "for element in [febr, march, data_april, data_may]:\n",
        "  element.date_ch = pd.to_datetime(element.date_ch)"
      ],
      "metadata": {
        "id": "SnaCOcFY4LBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#смотрим, что есть \n",
        "#febr.groupby('id_tt_cl').date_ch.agg([min, max])"
      ],
      "metadata": {
        "id": "GoBnbylZ6u_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#here is the example of february file \n",
        "start = pd.Timestamp('2022-02-01')\n",
        "end = start + DateOffset(weeks=1)\n",
        "mask = (febr.date_ch >= start) & (febr.date_ch <= end)\n",
        "febr.loc[mask].groupby('id_tt_cl').BaseSum.sum()"
      ],
      "metadata": {
        "id": "7-Ktztzf5ytT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we see that one id is missing \n",
        "print(np.sort(febr.loc[mask].id_tt_cl.unique()))\n",
        "print(np.sort(febr['id_tt_cl'].unique()))    "
      ],
      "metadata": {
        "id": "HQJ_qOFVAhYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#this id is 11783, let's consider what's wrong with it \n",
        "febr.loc[febr.id_tt_cl == 11783].date_ch.agg([min, max])\n",
        "#no information of this id in the first week of february"
      ],
      "metadata": {
        "id": "MFfazgyECPJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check if the information is full of the others\n",
        "start = pd.Timestamp('2022-02-08')\n",
        "end = start + DateOffset(weeks=1)\n",
        "for i in range(3):\n",
        "  tmp = (febr.date_ch >= start) & (febr.date_ch < end)\n",
        "  print(len(febr.loc[tmp].id_tt_cl.unique()))\n",
        "  start = end\n",
        "  end += DateOffset(weeks=1)\n",
        "#yes, this part is ok"
      ],
      "metadata": {
        "id": "IU5iwBDZpBnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create new table for february\n",
        "new_data = {'id_tt_cl':np.sort(febr['id_tt_cl'].unique())}\n",
        "new = pd.DataFrame(data=new_data)"
      ],
      "metadata": {
        "id": "Ec4RbhcK_da5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#index of missing id in first week\n",
        "ind = np.where(np.sort(febr['id_tt_cl'].unique())== 11783)\n",
        "print(ind)"
      ],
      "metadata": {
        "id": "tJUmz9oUmMS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fill 'manually' for 1st week of february  \n",
        "start = pd.Timestamp('2022-02-01')\n",
        "end = start + DateOffset(weeks=1) - DateOffset(days=1)\n",
        "mask = (febr.date_ch >= start) & (febr.date_ch < end)\n",
        "for i in range(84):\n",
        "  if i < 23:\n",
        "    new.at[i,'sum_week0'] = febr.loc[mask].groupby('id_tt_cl').BaseSum.sum().sort_index().values[i]\n",
        "  elif i == 23:\n",
        "    new.at[i,'sum_week0'] = 0 \n",
        "  elif i > 23:\n",
        "    new.at[i,'sum_week0'] = febr.loc[mask].groupby('id_tt_cl').BaseSum.sum().sort_index().values[i-1]"
      ],
      "metadata": {
        "id": "uouZ6R0Gk5E0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for other 3 week \n",
        "start = pd.Timestamp('2022-02-07')\n",
        "end = start + DateOffset(weeks=1)\n",
        "for i in range(3):\n",
        "  mask = (febr.date_ch >= start) & (febr.date_ch < end)\n",
        "  new['sum_week'+str(i+1)] = febr.loc[mask].groupby('id_tt_cl').BaseSum.sum().sort_index().values\n",
        "  start = end\n",
        "  end += DateOffset(weeks=1)\n",
        "new.head()"
      ],
      "metadata": {
        "id": "Cdo6pYlko391"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#since different months may belong to the same week, we join the tables and do the same\n",
        "data = pd.concat([march, data_april, data_may])"
      ],
      "metadata": {
        "id": "CiWsQQaJrrDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check if the information is full again \n",
        "start = pd.Timestamp('2022-03-01')\n",
        "end = start + DateOffset(weeks=1)\n",
        "for i in range(13):\n",
        "  tmp = (data.date_ch >= start) & (data.date_ch < end)\n",
        "  print(len(data.loc[tmp].id_tt_cl.unique())) \n",
        "  start = end\n",
        "  end += DateOffset(weeks=1)\n",
        "#it's ok"
      ],
      "metadata": {
        "id": "PqB-VsSWtc1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = pd.Timestamp('2022-03-01')\n",
        "end = start + DateOffset(weeks=1) - DateOffset(days=1)\n",
        "for i in range(3, 16):\n",
        "  mask = (data.date_ch >= start) & (data.date_ch < end)\n",
        "  new['sum_week'+str(i+1)] = data.loc[mask].groupby('id_tt_cl').BaseSum.sum().sort_index().values\n",
        "  start = end\n",
        "  end += DateOffset(weeks=1)\n",
        "new.head()"
      ],
      "metadata": {
        "id": "jHFFo7LJumsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arr_coef = np.array([0]*84, dtype=float)\n",
        "x = 10000*np.array(range(2,18)).reshape(-1, 1) \n",
        "for i in range(84):\n",
        "  y = new.iloc[i,1:17].values.reshape(-1, 1)\n",
        "  reg = LinearRegression()  \n",
        "  reg.fit(x, y)  \n",
        "  arr_coef[i] = reg.coef_\n",
        "new['coef'] = arr_coef\n",
        "#print(reg.coef_)\n",
        "#plt.scatter(x, y)\n",
        "#plt.plot(x, y_pred, color='red')\n",
        "#plt.show()\n",
        "new.head()"
      ],
      "metadata": {
        "id": "bxd6umQYxtSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new['coef'].describe()"
      ],
      "metadata": {
        "id": "qbpT134X5n1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new.hist(column='coef',bins=25)"
      ],
      "metadata": {
        "id": "ox7-x2dA55ew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#marking every id \n",
        "for i in range(84):\n",
        "  if new.loc[i, 'coef'] > 1.1:\n",
        "    new.at[i, 'revenue'] = 'good'\n",
        "  elif new.loc[i, 'coef'] < 0.9:\n",
        "    new.at[i,'revenue'] = 'bad'\n",
        "  else:\n",
        "    new.at[i,'revenue'] = 'nothing'\n",
        "new.head()"
      ],
      "metadata": {
        "id": "5Ee0Yyuhnvjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new.to_csv('/content/drive/MyDrive/Vkusvill/weeksum_v2.csv', index=False, sep = ';')"
      ],
      "metadata": {
        "id": "oTuZX-_uC3s5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check's count and average check "
      ],
      "metadata": {
        "id": "2fiSJyZzspfr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count_d = {'id_tt_cl':np.sort(febr['id_tt_cl'].unique())}\n",
        "count_check = pd.DataFrame(data=count_d)\n",
        "count_check.head()"
      ],
      "metadata": {
        "id": "nKIVwzXCCrrf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = pd.Timestamp('2022-02-01')\n",
        "end = start + DateOffset(weeks=1) - DateOffset(days=1)\n",
        "mask = (febr.date_ch >= start) & (febr.date_ch < end)\n",
        "febr.loc[mask].groupby(['id_tt_cl', 'CheckUID']).CheckUID.count()"
      ],
      "metadata": {
        "id": "bundKUcpoPud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = pd.Timestamp('2022-02-01')\n",
        "end = start + DateOffset(weeks=1) - DateOffset(days=1)\n",
        "mask = (febr.date_ch >= start) & (febr.date_ch < end)\n",
        "for i in range(84):\n",
        "  if i < 23:\n",
        "    count_check.at[i,'count_week0'] = len(febr.loc[mask].groupby('id_tt_cl').CheckUID.unique().sort_index().values[i])\n",
        "  elif i == 23:\n",
        "    count_check.at[i,'count_week0'] = 0 \n",
        "  elif i > 23:\n",
        "    count_check.at[i,'count_week0'] = len(febr.loc[mask].groupby('id_tt_cl').CheckUID.unique().sort_index().values[i-1])"
      ],
      "metadata": {
        "id": "p6bUSFJoDRNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = pd.Timestamp('2022-02-07')\n",
        "end = start + DateOffset(weeks=1)\n",
        "for i in range(3):\n",
        "  mask = (febr.date_ch >= start) & (febr.date_ch < end)\n",
        "  arr = np.array(len(febr.loc[mask].groupby('id_tt_cl').CheckUID.unique().sort_index().values[i]) for i in range(84))\n",
        "  count_check['count_week'+str(i+1)] = arr\n",
        "  start = end\n",
        "  end += DateOffset(weeks=1)\n",
        "count_check.head()"
      ],
      "metadata": {
        "id": "MPr63LdzEOPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = pd.Timestamp('2022-03-01')\n",
        "end = start + DateOffset(weeks=1) - DateOffset(days=1)\n",
        "for i in range(3, 16):\n",
        "  mask = (data.date_ch >= start) & (data.date_ch < end)\n",
        "  arr = np.array(len(data.loc[mask].groupby('id_tt_cl').CheckUID.unique().sort_index().values[i]) for i in range(84))\n",
        "  count_check['count_week'+str(i+1)] = arr\n",
        "  start = end\n",
        "  end += DateOffset(weeks=1)\n",
        "count_check.head()"
      ],
      "metadata": {
        "id": "bLkYXNynEoXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_check.to_csv('/content/drive/MyDrive/Vkusvill/count.csv', index=False)"
      ],
      "metadata": {
        "id": "vo6K4sAHFHno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_check = pd.read_csv('/content/drive/MyDrive/Vkusvill/count.csv')"
      ],
      "metadata": {
        "id": "TCAkbu9qud3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new = pd.read_csv('/content/drive/MyDrive/Vkusvill/weeksum_v2.csv', sep = ';')"
      ],
      "metadata": {
        "id": "bvwoCl3CvGxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "average_check_d = {'id_tt_cl':np.sort(count_check['id_tt_cl'].unique())} \n",
        "average_check = pd.DataFrame(data=average_check_d)"
      ],
      "metadata": {
        "id": "RenwydynFc-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(17):\n",
        "  average_check['av_check_week'+str(i)] = np.divide(new['sum_week'+str(i)].sort_index().values, count_check['count_week'+str(i)].sort_index().values)\n",
        "average_check.head()"
      ],
      "metadata": {
        "id": "LSQA4AW2FwVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "average_check.to_csv('/content/drive/MyDrive/Vkusvill/average.csv', index=False)"
      ],
      "metadata": {
        "id": "ECd4Wft1JKUF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}